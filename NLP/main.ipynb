{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05b9d422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, how are you doing today?', \"Today's weather great, and Python is awesome.\", 'The sky is blue and the sun is shining.']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')  # only 'punkt' is needed for sent_tokenize\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "corpus = \"\"\"Hello, how are you doing today? Today's weather great, and Python is awesome. The sky is blue and the sun is shining.\"\"\"\n",
    "\n",
    "sentences = sent_tokenize(corpus)\n",
    "print(sentences)\n",
    "print(type(sentences))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db32f752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'Today',\n",
       " \"'s\",\n",
       " 'weather',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'Python',\n",
       " 'is',\n",
       " 'awesome',\n",
       " '.',\n",
       " 'The',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'blue',\n",
       " 'and',\n",
       " 'the',\n",
       " 'sun',\n",
       " 'is',\n",
       " 'shining',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenization \n",
    "## Paragraph --> Words\n",
    "## Sentence --> Words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(corpus) # consider words as tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc28b756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n",
      "['Today', \"'s\", 'weather', 'great', ',', 'and', 'Python', 'is', 'awesome', '.']\n",
      "['The', 'sky', 'is', 'blue', 'and', 'the', 'sun', 'is', 'shining', '.']\n"
     ]
    }
   ],
   "source": [
    "for sen in sentences:\n",
    "    print(word_tokenize(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11b4ac04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'Today',\n",
       " \"'\",\n",
       " 's',\n",
       " 'weather',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'Python',\n",
       " 'is',\n",
       " 'awesome',\n",
       " '.',\n",
       " 'The',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'blue',\n",
       " 'and',\n",
       " 'the',\n",
       " 'sun',\n",
       " 'is',\n",
       " 'shining',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize ## considers punctuation as separate tokens\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a079dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'Today',\n",
       " \"'s\",\n",
       " 'weather',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'Python',\n",
       " 'is',\n",
       " 'awesome.',\n",
       " 'The',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'blue',\n",
       " 'and',\n",
       " 'the',\n",
       " 'sun',\n",
       " 'is',\n",
       " 'shining',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer() # here full stops are not considered as separate tokens except at the end of the sentence.\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35157ab6",
   "metadata": {},
   "source": [
    "## Stemming \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb24fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification problem\n",
    "\n",
    "\n",
    "words = [\"eating\",'eats','eaten','writing','writes','programming','program ','history','finally','finalize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81a2ed8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eats ---> eat\n",
      "eaten ---> eaten\n",
      "writing ---> write\n",
      "writes ---> write\n",
      "programming ---> program\n",
      "program  ---> program \n",
      "history ---> histori\n",
      "finally ---> final\n",
      "finalize ---> final\n"
     ]
    }
   ],
   "source": [
    "## Porter Stemmer \n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "for word in words:\n",
    "    print(word+\" ---> \" +stemming.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "721ef01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('congratulation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c616f217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('sitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0172a270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratulate'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Regex based stemming\n",
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|es$|ed$', min=4)\n",
    "reg_stemmer.stem('congratulate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f03fe6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eats --> eat\n",
      "eaten --> eaten\n",
      "writing --> write\n",
      "writes --> write\n",
      "programming --> program\n",
      "program  --> program \n",
      "history --> histori\n",
      "finally --> final\n",
      "finalize --> final\n"
     ]
    }
   ],
   "source": [
    "## Snowball stemmer\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snow = SnowballStemmer('english')\n",
    "for word in words:\n",
    "    print(word+ \" --> \" +snow.stem(word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
